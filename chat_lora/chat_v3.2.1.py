import torch
import json
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

MODEL_PATH = "models/Qwen2.5-3B-Instruct"
LORA_PATH = "lora_aigf_v3.2"
MEMORY_FILE = "aria_memory.json"
DEVICE = "mps"

class AriaChat:
    def __init__(self):
        print(f"âœ¨ æ­£åœ¨å”¤é†’ Aria (V3.2)...")
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        base_model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH, torch_dtype=torch.bfloat16
        ).to(DEVICE)
        self.model = PeftModel.from_pretrained(base_model, LORA_PATH)
        self.model.eval()

        self.memory = self.load_memory()
        self.history = self.init_system_prompt()

    def load_memory(self):
        if os.path.exists(MEMORY_FILE):
            with open(MEMORY_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {"user_name": "ä½ ", "key_events": []}

    def init_system_prompt(self):

        memory_str = f"ä½ çš„åå­—å« Ariaã€‚ä½ æ˜¯ç”¨æˆ·çš„äº²å¯†ä¼´ä¾£ã€‚ç”¨æˆ·å«{self.memory['user_name']}ã€‚"
        if self.memory['key_events']:
            events = "ï¼Œ".join(self.memory['key_events'][-3:]) # åªå–æœ€è¿‘3æ¡é‡è¦è®°å¿†
            memory_str += f"ä½ è¿˜è®°å¾—ï¼š{events}ã€‚"
            
        return [
            {"role": "system", "content": f"{memory_str} ä½ æ€§æ ¼æ¸©æŸ”éšæ€§ï¼Œå–œæ¬¢ç”¨è¡¨æƒ…åŒ…ã€‚æˆ‘ä»¬ç°åœ¨åœ¨å®¶é‡Œã€‚è¯´è¯ç®€çŸ­ï¼Œä¸¥ç¦æåŠå­¦æ ¡ã€æ‰“è½¦ç­‰çäº‹ã€‚"}
        ]

    def chat(self):
        print(f"\n--- Aria å·²ä¸Šçº¿ (å½“å‰è®°å¿†: {self.memory['user_name']}) ---")
        while True:
            user_input = input("\næˆ‘ï¼š")
            if user_input.lower() in ["exit", "quit"]: break
            
            if "å«æˆ‘" in user_input:
                new_name = user_input.split("å«æˆ‘")[-1].strip(" ï¼Œã€‚")
                self.memory['user_name'] = new_name
                print(f"ğŸ’¡ Aria è®°ä½äº†ï¼Œä»¥åå«ä½  {new_name}")
                continue

            self.history.append({"role": "user", "content": user_input})
            
            prompt = self.tokenizer.apply_chat_template(self.history, tokenize=False, add_generation_prompt=True)
            inputs = self.tokenizer(prompt, return_tensors="pt").to(DEVICE)

            with torch.no_grad():
                output_ids = self.model.generate(
                    **inputs, max_new_tokens=64, do_sample=True,
                    temperature=0.6, top_p=0.8, repetition_penalty=1.2
                )

            response = self.tokenizer.decode(output_ids[0][len(inputs["input_ids"][0]):], skip_special_tokens=True)
            print(f"Ariaï¼š{response}")
            self.history.append({"role": "assistant", "content": response})
            
            if len(self.history) > 11:
                self.history = [self.history[0]] + self.history[-10:]

    def save_and_exit(self):
        with open(MEMORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(self.memory, f, ensure_ascii=False)
        print("ğŸŒ™ Aria å»ä¼‘æ¯äº†ï¼Œè®°å¿†å·²ä¿å­˜ã€‚")

if __name__ == "__main__":
    aria = AriaChat()
    try:
        aria.chat()
    finally:
        aria.save_and_exit()
