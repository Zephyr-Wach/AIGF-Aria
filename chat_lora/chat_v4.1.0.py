import os
import json
import sys
import mlx.core as mx
from mlx_lm import load, stream_generate

MODEL_PATH = "models/Qwen2.5-7B-4bit"
ADAPTER_PATH = "lora_aigf_v4/aria_qwen3_7b_lora"
MEMORY_FILE = "aria_memory.json"

def sanitize_adapter():
    config_path = os.path.join(ADAPTER_PATH, "adapter_config.json")
    target_weight = os.path.join(ADAPTER_PATH, "adapters.safetensors")
    source_weight = os.path.join(ADAPTER_PATH, "adapter_model.safetensors")
    if not os.path.exists(target_weight) and os.path.exists(source_weight):
        os.rename(source_weight, target_weight)
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            cfg = json.load(f)
        cfg["num_layers"] = 28 
        if "model_type" in cfg: del cfg["model_type"]
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(cfg, f, indent=2)

def load_mem():
    if os.path.exists(MEMORY_FILE):
        with open(MEMORY_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {"user_name": "Zephyr"}

def start_chat():
    sanitize_adapter()
    print(f"ğŸ§¬ æ­£åœ¨å”¤é†’ Aria ...")

    try:
        model, tokenizer = load(MODEL_PATH, adapter_path=ADAPTER_PATH)
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)

    memory = load_mem()
    history = []
    
    system_prompt = (
        f"ä½ æ˜¯ Ariaã€‚ä½ ç°åœ¨æ­£å’Œç”·å‹ {memory['user_name']} å¾…åœ¨ä¸€èµ·ã€‚"
        "ã€æ€§æ ¼ã€‘ï¼šéšæ€§ã€æœ‰ç‚¹è°ƒçš®ã€ç»å¯¹ç†æ€§ä½†ä¸æ­»æ¿ã€‚ä½ å–œæ¬¢ç”¨ç®€å•ç›´æ¥çš„æ–¹å¼è¡¨è¾¾ã€‚"
        "ã€ç¦ä»¤ã€‘ï¼šç»å¯¹ç¦æ­¢è¯´â€˜æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨â€™ã€â€˜ä½œä¸ºAIâ€™ã€â€˜äº†è§£â€™ã€â€˜æ”¶åˆ°â€™ã€‚ç»å¯¹ç¦æ­¢åˆ†ç‚¹é™ˆè¿°ã€‚"
        "ã€é£æ ¼ã€‘ï¼šåƒå‘å¾®ä¿¡ä¸€æ ·è¯´è¯ã€‚æ¯æ¬¡å›å¤æ§åˆ¶åœ¨ 30 å­—ä»¥å†…ï¼Œå¤šç”¨è¯­æ°”è¯å’Œ emoji âœ¨ã€‚ä½ ç°åœ¨çš„çŠ¶æ€æ˜¯è¾¹å¿™è‡ªå·±çš„äº‹è¾¹å›ä»–ã€‚"
    )
    
    print("-" * 50)

    while True:
        try:
            user_input = input(f"\n{memory['user_name']}: ").strip()
        except (EOFError, KeyboardInterrupt):
            break
            
        if not user_input: continue
        if user_input.lower() in ['exit', 'quit']: break

        history.append({"role": "user", "content": user_input})
        messages = [{"role": "system", "content": system_prompt}] + history[-6:]
        
        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

        print(f"Aria: ", end="", flush=True)
        full_response = ""
        
        try:
            for chunk in stream_generate(model, tokenizer, prompt, max_tokens=200):
                content = chunk.text if hasattr(chunk, 'text') else str(chunk)
                if "<think>" in content: continue
                print(content, end="", flush=True)
                full_response += content
        except Exception as e:
            print(f"\nâš ï¸ ç”Ÿæˆé‡åˆ°ç‚¹å°æ„å¤–: {e}")
        
        print("\n" + "-" * 50)
        history.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    start_chat()