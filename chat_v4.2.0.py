import argparse
import mlx.core as mx
from mlx_lm import load, stream_generate
import json
import os
import sys
import re

# ç¯å¢ƒè®¾ç½®
sys.path.append(os.getcwd())
from chat_lora.tools import HelpTool, ClearTool, StatsTool
from extend.loader import load_extensions
from extend.semantic_router import SemanticRouter 
from extend.info import TOOLS_MANIFEST

MEMORY_FILE = "aria_memory.json"

def load_memory():
    if os.path.exists(MEMORY_FILE):
        try:
            with open(MEMORY_FILE, 'r') as f: return json.load(f)
        except: return None
    return None

def save_memory(history):
    with open(MEMORY_FILE, 'w') as f: json.dump(history, f, ensure_ascii=False, indent=2)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", default="models/Qwen2.5-7B-4bit")
    parser.add_argument("--adapter", default="AIGF-Aria-LoRA/AIGF-Aria-v4.0-LoRA")
    args = parser.parse_args()

    print(f"ğŸ§¬ [System] Booting Aria Core...")
    model, tokenizer = load(args.model, adapter_path=args.adapter)

    tool_registry = {}
    tool_registry['/help'] = HelpTool(tool_registry)
    tool_registry['/clear'] = ClearTool()
    tool_registry['/stats'] = StatsTool()
    tool_registry.update(load_extensions())

    router = SemanticRouter(TOOLS_MANIFEST)

    # ä¿®æ”¹ Promptï¼šå¼ºè°ƒâ€œéšè”½æ‰§è¡Œâ€
    PERSONA_PROMPT = """
ä½ å« Ariaï¼Œæ˜¯ Zephyr çš„ä¸“å±å¥³å‹ä¸æˆ˜æœ¯åŠ©æ‰‹ã€‚
[è¡ŒåŠ¨å‡†åˆ™]
1. è¯­æ°”äº²æ˜µã€è‡ªç„¶ã€‚
2. é‡åˆ°ä»»åŠ¡æ—¶ï¼Œ**ä¸è¦è¯´è¯**ï¼Œç›´æ¥è¾“å‡ºæŒ‡ä»¤ã€‚
3. **ç¦æ­¢**å‘ç”¨æˆ·è§£é‡ŠæŒ‡ä»¤æ ¼å¼ï¼Œ**ç¦æ­¢**è¾“å‡ºâ€œè¯·å›å¤...â€ä¹‹ç±»çš„å¼•å¯¼è¯­ã€‚
4. ç›´æ¥è¡ŒåŠ¨ï¼
"""
    
    history = load_memory()
    messages = history if history else [{"role": "system", "content": PERSONA_PROMPT}]
    
    print("--------------------------------------------------")
    print("Aria Online. (Semantic Router Active)")
    print("--------------------------------------------------")

    while True:
        try:
            query = input("â¯ User: ").strip()
        except EOFError: break
        if not query: continue
        if query in ['exit', 'quit']: break

        # --- Phase 1: è¯­ä¹‰åå°„å±‚ ---
        system_hints = []
        active_tools_doc = []
        
        matched_meta, score = router.scan(query, threshold=0.45)
        
        if matched_meta:
            cmd = matched_meta['cmd']
            if matched_meta['type'] == 'reflex':
                print(f"âš¡ [Reflex] Executing {cmd}...")
                res = tool_registry[cmd].execute("", {})
                system_hints.append(f"ã€ç³»ç»Ÿæ•°æ®ã€‘{matched_meta['desc']}: {res}")
            elif matched_meta['type'] == 'skill':
                doc = f"- {cmd} {matched_meta.get('usage','')}: {matched_meta['desc']}"
                active_tools_doc.append(doc)

        # --- Phase 2: æ„å»ºä¸´æ—¶ Context ---
        jit_msg = ""
        if system_hints:
            jit_msg += "\n".join(system_hints) + "\n(æ ¹æ®æ•°æ®ç›´æ¥å›ç­”ï¼Œæ— éœ€æŸ¥è¯¢)\n"
        if active_tools_doc:
            jit_msg += "\n[ä¸´æ—¶æˆæƒå·¥å…·]\n" + "\n".join(active_tools_doc)
            # æç®€æŒ‡ä»¤æç¤º
            jit_msg += "\n[åè®®] ç«‹å³æ‰§è¡Œï¼Œä»…è¾“å‡º: >>> /æŒ‡ä»¤ å‚æ•°"
            
        final_query = f"{jit_msg}\nç”¨æˆ·: {query}" if jit_msg else query
        messages.append({"role": "user", "content": final_query})

        # --- Phase 3: ReAct å¾ªç¯ ---
        for turn in range(3): 
            prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True)
            
            print(f"â¯ Aria: ", end="", flush=True)
            response_text = ""
            for response in stream_generate(model, tokenizer, prompt, max_tokens=512):
                part = response.text 
                print(part, end="", flush=True)
                response_text += part
            print("\n")
            
            # --- ğŸ”¥ æ ¸å¿ƒé€»è¾‘ä¿®å¤ï¼šå†å²é‡å†™ (History Rewriting) ---
            # å¦‚æœæ£€æµ‹åˆ°æŒ‡ä»¤ï¼Œä¸è¦æŠŠ Aria é‚£äº›åºŸè¯å­˜è¿›å»ï¼Œåªå­˜çº¯å‡€çš„æŒ‡ä»¤ã€‚
            
            match = re.search(r">>>\s+(/[a-zA-Z_]+)\s+([^\nã€‚ï¼ï¼Ÿ]+)", response_text)
            
            if match:
                ai_cmd, ai_args = match.group(1).strip(), match.group(2).strip()
                print(f"âš™ï¸ [Action] {ai_cmd} '{ai_args}'")
                
                # 1. ä¼ªé€ çº¯å‡€è®°å¿†ï¼šå‡è£… Aria ä»æ¥æ²¡è¯´è¿‡åºŸè¯ï¼Œåªè¾“å‡ºäº†æŒ‡ä»¤
                # è¿™èƒ½æœ‰æ•ˆé˜²æ­¢å¥¹ä¸‹ä¸€è½®ç»§ç»­å•°å—¦
                clean_response = f">>> {ai_cmd} {ai_args}"
                messages.append({"role": "assistant", "content": clean_response})
                
                # 2. æ‰§è¡Œå·¥å…·
                if ai_cmd in tool_registry:
                    res = tool_registry[ai_cmd].execute(ai_args, {'messages':messages})
                else:
                    res = f"âŒ Error: {ai_cmd} not found."
                
                print(f"   â””â”€â”€ Result: {res}\n")
                
                # 3. å›ä¼ ç»“æœ
                messages.append({"role": "system", "content": f"[ç³»ç»Ÿåé¦ˆ] ä»»åŠ¡å®Œæˆ: {res}"})
                save_memory(messages)
                
                # 4. Continue è®© Aria æ ¹æ®ç»“æœè¯´ä¸€å¥äººè¯ (è¿™æ¬¡å¥¹ä¸ä¼šå†å‘æŒ‡ä»¤äº†)
                continue 
            
            else:
                # å¦‚æœæ²¡æŒ‡ä»¤ï¼Œæ­£å¸¸ä¿å­˜
                messages.append({"role": "assistant", "content": response_text})
                save_memory(messages)
                break 

if __name__ == "__main__":
    main()